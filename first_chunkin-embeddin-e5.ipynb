{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12711050,"sourceType":"datasetVersion","datasetId":8033756}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"e2f983b5","cell_type":"code","source":"# Uncomment the following lines if `transformers` or `torch` is not available\n!pip install --quiet transformers torch chromadb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T18:52:32.855498Z","iopub.execute_input":"2025-08-09T18:52:32.856067Z","iopub.status.idle":"2025-08-09T18:54:07.317106Z","shell.execute_reply.started":"2025-08-09T18:52:32.856033Z","shell.execute_reply":"2025-08-09T18:54:07.316415Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.6/19.6 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.1/321.1 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 6.31.1 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 6.31.1 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.31.1 which is incompatible.\ngoogle-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\ndataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.31.1 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"id":"9b6852a5","cell_type":"code","source":"\nimport json\nimport pickle\nfrom pathlib import Path\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\nimport numpy as np\n\n# Use a retrieval‑optimised model (E5 base)\nMODEL_NAME = 'intfloat/e5-base-v2'\n\n# Chunk parameters\nCHUNK_SIZE = 128        # tokens per chunk\nCHUNK_OVERLAP = 32      # token overlap between chunks\n\n# Prefixes for E5 model\nPASSAGE_PREFIX = 'passage: '\nQUERY_PREFIX = 'query: '\n\n# Paths\nRAW_PATH = Path('/kaggle/input/ukyfkyugj/hajjjjjjjj.txt')  # adjust if needed\nJSON_OUTPUT = Path('hajj_chunks_e5.json')\nPKL_OUTPUT = Path('hajj_chunks_e5.pkl')\nNPY_OUTPUT = Path('hajj_embeddings_e5.npy')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T18:54:07.318484Z","iopub.execute_input":"2025-08-09T18:54:07.318707Z","iopub.status.idle":"2025-08-09T18:54:13.791778Z","shell.execute_reply.started":"2025-08-09T18:54:07.318685Z","shell.execute_reply":"2025-08-09T18:54:13.790878Z"}},"outputs":[],"execution_count":2},{"id":"eb78146d","cell_type":"code","source":"\n# Load the tokenizer and model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModel.from_pretrained(MODEL_NAME)\nmodel.to(device)\nmodel.eval()\n\n# Read raw text\ntext = RAW_PATH.read_text(encoding='utf-8').strip()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T18:54:51.120598Z","iopub.execute_input":"2025-08-09T18:54:51.121506Z","iopub.status.idle":"2025-08-09T18:54:51.707476Z","shell.execute_reply.started":"2025-08-09T18:54:51.121478Z","shell.execute_reply":"2025-08-09T18:54:51.706943Z"}},"outputs":[],"execution_count":4},{"id":"78d87a18","cell_type":"code","source":"\n# Tokenize entire document (for splitting by tokens)\ninputs = tokenizer(text, return_tensors='pt', add_special_tokens=False)\ntokens = inputs['input_ids'][0]\ntotal_tokens = len(tokens)\nprint(f\"Total tokens: {total_tokens}\")\n\n# Generate chunk boundaries\nstep = CHUNK_SIZE - CHUNK_OVERLAP\nchunks = []\nembeddings = []\n\nfor idx, start in enumerate(range(0, total_tokens, step)):\n    end = min(start + CHUNK_SIZE, total_tokens)\n    chunk_tokens = tokens[start:end]\n    # Decode to text\n    chunk_text = tokenizer.decode(chunk_tokens, skip_special_tokens=True)\n    # Prefix for passage\n    input_text = PASSAGE_PREFIX + chunk_text\n    encoded_input = tokenizer(input_text, return_tensors='pt', truncation=True, max_length=512)\n    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n    with torch.no_grad():\n        out = model(**encoded_input)\n        # Mean pooling\n        token_embeds = out.last_hidden_state  # (1, seq_len, hidden_dim)\n        mask = encoded_input['attention_mask'].unsqueeze(-1)\n        sum_embeds = (token_embeds * mask).sum(dim=1)\n        sum_mask = mask.sum(dim=1)\n        embed = (sum_embeds / sum_mask).squeeze(0).cpu().numpy()\n    # Normalize embedding\n    norm = np.linalg.norm(embed)\n    if norm > 0:\n        embed = embed / norm\n    # Append\n    chunks.append({\n        'chunk_id': idx,\n        'start_token': int(start),\n        'end_token': int(end),\n        'text': chunk_text\n    })\n    embeddings.append(embed)\n    if end == total_tokens:\n        break\n\nprint(f\"Generated {len(chunks)} chunks.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T18:55:07.134230Z","iopub.execute_input":"2025-08-09T18:55:07.135039Z","iopub.status.idle":"2025-08-09T18:55:11.987700Z","shell.execute_reply.started":"2025-08-09T18:55:07.135013Z","shell.execute_reply":"2025-08-09T18:55:11.987109Z"}},"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (38365 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"Total tokens: 38365\nGenerated 400 chunks.\n","output_type":"stream"}],"execution_count":5},{"id":"425b3373","cell_type":"code","source":"\n# Save JSON and pickle\nwith JSON_OUTPUT.open('w', encoding='utf-8') as f:\n    json.dump(chunks, f, ensure_ascii=False, indent=2)\nprint(f\"Saved {len(chunks)} chunks to {JSON_OUTPUT.resolve()}\")\n\nwith PKL_OUTPUT.open('wb') as f:\n    pickle.dump(chunks, f)\nprint(f\"Saved pickle to {PKL_OUTPUT.resolve()}\")\n\n# Save embeddings\nemb_array = np.vstack(embeddings)\nnp.save(NPY_OUTPUT, emb_array)\nprint(f\"Saved embeddings array to {NPY_OUTPUT.resolve()} with shape {emb_array.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T18:55:44.317871Z","iopub.execute_input":"2025-08-09T18:55:44.318149Z","iopub.status.idle":"2025-08-09T18:55:44.332334Z","shell.execute_reply.started":"2025-08-09T18:55:44.318130Z","shell.execute_reply":"2025-08-09T18:55:44.331697Z"}},"outputs":[{"name":"stdout","text":"Saved 400 chunks to /kaggle/working/hajj_chunks_e5.json\nSaved pickle to /kaggle/working/hajj_chunks_e5.pkl\nSaved embeddings array to /kaggle/working/hajj_embeddings_e5.npy with shape (400, 768)\n","output_type":"stream"}],"execution_count":6},{"id":"38b552d1-7b6f-4056-ae16-9a83beb3dbba","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}