{"cells":[{"cell_type":"code","execution_count":1,"id":"e069b078","metadata":{"execution":{"iopub.execute_input":"2025-08-10T07:12:07.697007Z","iopub.status.busy":"2025-08-10T07:12:07.696760Z","iopub.status.idle":"2025-08-10T07:15:51.233152Z","shell.execute_reply":"2025-08-10T07:15:51.232302Z","shell.execute_reply.started":"2025-08-10T07:12:07.696989Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.6/19.6 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.1/321.1 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n","google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 6.31.1 which is incompatible.\n","google-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 6.31.1 which is incompatible.\n","pandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n","google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.31.1 which is incompatible.\n","google-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\n","dataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\n","tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.31.1 which is incompatible.\n","bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n","bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["# Install required packages (if not already installed)\n","!pip install --quiet chromadb transformers torch llama-cpp-python\n"]},{"cell_type":"code","execution_count":2,"id":"0803595c","metadata":{"execution":{"iopub.execute_input":"2025-08-10T07:15:51.234903Z","iopub.status.busy":"2025-08-10T07:15:51.234674Z","iopub.status.idle":"2025-08-10T07:16:00.216477Z","shell.execute_reply":"2025-08-10T07:16:00.215332Z","shell.execute_reply.started":"2025-08-10T07:15:51.234880Z"},"trusted":true},"outputs":[],"source":["\n","import os\n","os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n","\n","import json\n","import numpy as np\n","import torch\n","from transformers import AutoTokenizer, AutoModel\n","import chromadb\n","from llama_cpp import Llama\n","\n","# Config for embedding model and Chroma\n","MODEL_NAME = 'intfloat/e5-base-v2'\n","CHROMA_PATH = '/kaggle/input/database-haj/hajj_e5_chroma_backup'\n","COLLECTION_NAME = 'hajj_e5'\n","PASSAGE_PREFIX = 'passage: '\n","QUERY_PREFIX = 'query: '\n","JSON_PATH = '/kaggle/input/database-haj/hajj_chunks_e5.json'\n","NPY_PATH = '/kaggle/input/database-haj/hajj_embeddings_e5.npy'\n","\n","# Path to your quantised LLM file (gguf format)\n","#LLM_PATH = '/path/to/mistral-7b-instruct-q4_k_m.gguf'  # TODO: replace with actual path on your Pi\n","\n","# Maximum tokens for generation and context\n","MAX_TOKENS = 256\n","\n","# Device selection\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"]},{"cell_type":"code","execution_count":3,"id":"8227b2fc","metadata":{"execution":{"iopub.execute_input":"2025-08-10T07:16:00.218180Z","iopub.status.busy":"2025-08-10T07:16:00.217566Z","iopub.status.idle":"2025-08-10T07:16:25.071121Z","shell.execute_reply":"2025-08-10T07:16:25.070354Z","shell.execute_reply.started":"2025-08-10T07:16:00.218143Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c1e2244d862c428781ad70bb52678817","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"77ae7aca5ad04defa9572d66a1511045","version_major":2,"version_minor":0},"text/plain":["vocab.txt: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3609222dc9ec406e8b82fee6799990d1","version_major":2,"version_minor":0},"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3a3892cb78cf46d2bff4e5ecbd7c6a28","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"73644ccc2984435499a5bb36cd463288","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/650 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2025-08-10 07:16:11.811993: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1754810172.019529      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1754810172.081673      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"]},{"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"]},{"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"]},{"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"]},{"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"081b071b59eb442496d4a734ec718fa4","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["\n","# Load E5 model and tokenizer for query encoding\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n","model.eval()\n","\n","def embed_query(text: str):\n","    \"\"\"Encode a query string into an embedding vector using E5 and normalise it.\"\"\"\n","    input_text = QUERY_PREFIX + text\n","    encoded = tokenizer(input_text, return_tensors='pt', truncation=True, max_length=512)\n","    encoded = {k: v.to(device) for k, v in encoded.items()}\n","    with torch.no_grad():\n","        out = model(**encoded)\n","        token_embeds = out.last_hidden_state\n","        mask = encoded['attention_mask'].unsqueeze(-1)\n","        sum_embeds = (token_embeds * mask).sum(dim=1)\n","        sum_mask = mask.sum(dim=1)\n","        embed = (sum_embeds / sum_mask).squeeze(0).cpu().numpy()\n","    norm = np.linalg.norm(embed)\n","    if norm > 0:\n","        embed = embed / norm\n","    return embed\n"]},{"cell_type":"code","execution_count":4,"id":"480376f5","metadata":{"execution":{"iopub.execute_input":"2025-08-10T07:16:25.072585Z","iopub.status.busy":"2025-08-10T07:16:25.071961Z","iopub.status.idle":"2025-08-10T07:16:25.536135Z","shell.execute_reply":"2025-08-10T07:16:25.535639Z","shell.execute_reply.started":"2025-08-10T07:16:25.072558Z"},"trusted":true},"outputs":[],"source":["\n","# Copy the folder into a writable location (if it came from a read-only dataset)\n","import shutil\n","shutil.copytree('/kaggle/input/database-haj/hajj_e5_chroma_backup', '/kaggle/working/hajj_e5_chroma')\n","\n","# Then point Chroma at the copy\n","CHROMA_PATH = '/kaggle/working/hajj_e5_chroma'\n","client = chromadb.PersistentClient(path=CHROMA_PATH)\n","collection = client.get_or_create_collection(name='hajj_e5', metadata={'hnsw:space': 'cosine'})\n","\n","# Helper search function with lexical re-ranking as fallback\n","def search(query_str: str, top_k: int = 10, re_rank: bool = True):\n","    query_embed = embed_query(query_str)\n","    result = collection.query(query_embeddings=[query_embed.tolist()], n_results=top_k)\n","    ids = result['ids'][0]\n","    dists = result['distances'][0]\n","    docs = result['documents'][0]\n","    metas = result['metadatas'][0]\n","    hits = []\n","    for id_, dist, doc, meta in zip(ids, dists, docs, metas):\n","        hits.append({'id': id_, 'distance': float(dist), 'text': doc, 'metadata': meta})\n","    if re_rank:\n","        query_tokens = set(query_str.lower().split())\n","        for h in hits:\n","            text_tokens = set(h['text'].lower().split())\n","            h['lexical_score'] = len(query_tokens & text_tokens)\n","        hits.sort(key=lambda x: x['lexical_score'], reverse=True)\n","    return hits\n"]},{"cell_type":"code","execution_count":5,"id":"da13e72c","metadata":{"execution":{"iopub.execute_input":"2025-08-10T07:16:25.537916Z","iopub.status.busy":"2025-08-10T07:16:25.537722Z","iopub.status.idle":"2025-08-10T07:16:25.542851Z","shell.execute_reply":"2025-08-10T07:16:25.542220Z","shell.execute_reply.started":"2025-08-10T07:16:25.537901Z"},"trusted":true},"outputs":[],"source":["\n","def build_prompt(question: str, sources: list):\n","    \"\"\"Construct a prompt for the LLM using the question and retrieved sources.\"\"\"\n","    prompt_lines = []\n","    prompt_lines.append(\"You are an assistant answering questions about Hajj and Umrah.\")\n","    prompt_lines.append(\"Answer concisely in plain English so that the response can be read aloud.\")\n","    prompt_lines.append(\"Keep the answer to no more than 3–4 sentences.\")\n","    prompt_lines.append(f\"Question: {question}\")\n","    prompt_lines.append(\"Sources:\")\n","    for i, src in enumerate(sources, 1):\n","        text = src['text'].replace(\"\", \" \").strip()\n","        if len(text) > 300:\n","            text = text[:297] + '...'\n","        prompt_lines.append(f\"[{i}] {text}\")\n","    prompt_lines.append(\"Answer:\")\n","    return \"\".join(prompt_lines)\n"]},{"cell_type":"code","execution_count":6,"id":"f00b3a5b-0454-4adf-aad1-357fd6cd5ae7","metadata":{"execution":{"iopub.execute_input":"2025-08-10T07:16:25.543738Z","iopub.status.busy":"2025-08-10T07:16:25.543508Z","iopub.status.idle":"2025-08-10T07:16:28.664904Z","shell.execute_reply":"2025-08-10T07:16:28.664015Z","shell.execute_reply.started":"2025-08-10T07:16:25.543717Z"},"trusted":true},"outputs":[],"source":["!pip -q install llama-cpp-python huggingface_hub"]},{"cell_type":"code","execution_count":17,"id":"b38efaae-b853-4d66-b071-1179cf61f20e","metadata":{"execution":{"iopub.execute_input":"2025-08-10T07:58:37.728311Z","iopub.status.busy":"2025-08-10T07:58:37.727798Z","iopub.status.idle":"2025-08-10T07:58:37.780390Z","shell.execute_reply":"2025-08-10T07:58:37.779832Z","shell.execute_reply.started":"2025-08-10T07:58:37.728291Z"},"trusted":true},"outputs":[],"source":["from openai import OpenAI\n","client = OpenAI(api_key=\"\")"]},{"cell_type":"code","execution_count":18,"id":"5f2446fb-ee1f-4136-933e-d8bc152e1874","metadata":{"execution":{"iopub.execute_input":"2025-08-10T07:58:42.265438Z","iopub.status.busy":"2025-08-10T07:58:42.265157Z","iopub.status.idle":"2025-08-10T07:58:42.270093Z","shell.execute_reply":"2025-08-10T07:58:42.269411Z","shell.execute_reply.started":"2025-08-10T07:58:42.265419Z"},"trusted":true},"outputs":[],"source":["def generate_answer(question: str, top_k: int = 5):\n","    hits = search(question, top_k=top_k, re_rank=True)\n","    prompt = build_prompt(question, hits)\n","    result = client.chat.completions.create(\n","        model=\"gpt-4o-mini\",   # or \"gpt-4o\", \"gpt-4-turbo\"\n","        messages=[\n","            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n","            {\"role\": \"user\", \"content\": prompt},\n","        ],\n","        temperature=0.2,\n","        max_tokens=MAX_TOKENS,\n","        stop=[\"Sources:\", \"Question:\"]\n","    )\n","    answer = result.choices[0].message.content.strip()\n","    return answer, hits"]},{"cell_type":"code","execution_count":21,"id":"2b3d9e29","metadata":{"execution":{"iopub.execute_input":"2025-08-10T08:00:05.983348Z","iopub.status.busy":"2025-08-10T08:00:05.983055Z","iopub.status.idle":"2025-08-10T08:00:08.225400Z","shell.execute_reply":"2025-08-10T08:00:08.224675Z","shell.execute_reply.started":"2025-08-10T08:00:05.983328Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Answer: Tawaf starts from the Black Stone, known as Al-Hajar Al-Aswad, which is located at one corner of the Kaaba. Pilgrims begin their circumambulation by facing the Black Stone and ideally touching or pointing to it. They then walk around the Kaaba seven times in a counterclockwise direction.\n"]}],"source":["\n","# Example question\n","question = \"from where tawaf starts?\"\n","answer, sources = generate_answer(question, top_k=5)\n","print(\"Answer:\", answer)\n"]},{"cell_type":"code","execution_count":22,"id":"78fc7a19-0803-4fde-9fec-918344eb2575","metadata":{"execution":{"iopub.execute_input":"2025-08-10T08:00:16.375301Z","iopub.status.busy":"2025-08-10T08:00:16.374678Z","iopub.status.idle":"2025-08-10T08:00:16.379669Z","shell.execute_reply":"2025-08-10T08:00:16.378890Z","shell.execute_reply.started":"2025-08-10T08:00:16.375270Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Sources used:\n","[1] - don't push in crowds - avoid placing feet on side brushes tawaf detailed instructions starting and ending tawaf : - begin from the black stone corne\n","[2] performing tawaf. recite surah al - kaafiroon in first rak'aa and surah al - ikhlaas in second. drink zamzam water after tawaf as the prophet did. sa'\n","[3] but difficult during crowded times - continue supplicating allah throughout tawaf - touch the yemeni corner if possible when reached - end each circui\n","[4] before leaving makkah, in compliance with the prophet's command for pilgrims to make tawaf their last engagement with the ka'ba. exceptions : - those \n","[5] and is also called tawaf az - ziyaarah or tawaf al - hajj. one should circumambulate the ka'ba seven times. allah almighty said : \" then let them comp\n"]}],"source":["print(\"Sources used:\")\n","\n","for i, src in enumerate(sources, 1):\n","    print(f\"[{i}] {src['text'][:150].replace('','')}\")"]},{"cell_type":"code","execution_count":23,"id":"0abe9f98-0619-4c96-95c5-48ba2a283f5d","metadata":{"execution":{"iopub.execute_input":"2025-08-10T08:00:18.231666Z","iopub.status.busy":"2025-08-10T08:00:18.231019Z","iopub.status.idle":"2025-08-10T08:00:21.696589Z","shell.execute_reply":"2025-08-10T08:00:21.696003Z","shell.execute_reply.started":"2025-08-10T08:00:18.231638Z"},"trusted":true},"outputs":[],"source":["from pathlib import Path\n","from openai import OpenAI\n","import os\n","\n","api_key = \"\"\n","\n","client = OpenAI(api_key=api_key)\n","speech_file_path = \"/kaggle/working/speech.mp3\"\n","\n","with client.audio.speech.with_streaming_response.create(\n","    model=\"gpt-4o-mini-tts\",\n","    voice=\"coral\",\n","    input=answer,\n","    instructions=\"normal and friendly\",\n",") as response:\n","    response.stream_to_file(speech_file_path)"]},{"cell_type":"code","execution_count":null,"id":"01d9373e-54ec-4f9f-9d51-f1141e9d39db","metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":8040723,"sourceId":12721500,"sourceType":"datasetVersion"}],"dockerImageVersionId":31090,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"}},"nbformat":4,"nbformat_minor":5}
