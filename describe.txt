At a high level this project delivers a complete, production‑ready RAG assistant for Hajj and Umrah guidance that can run entirely on a Raspberry Pi 5. We began by ingesting a large PDF and text corpus describing rituals, locations, health‑and‑safety advice and emergency procedures, then applied a careful chunking strategy (128‑token windows with overlap) so that each passage captured just enough context. These chunks were encoded with the retrieval‑optimised intfloat/e5‑base‑v2 model—prefixed as passage: or query: and normalised to unit length—and stored in a Chroma vector database configured for cosine similarity. Chroma’s lightweight persistent index uses only a few megabytes of RAM
cookbook.chromadb.dev
, making it well‑suited to a Pi, and its Python API made it easy to load the 400 embeddings and query them. We wrapped this with a retrieval function that embeds incoming questions, returns the top‑K matches and optionally re‑ranks them using a lexical overlap heuristic or a small cross‑encoder. This ensures that questions like “Tawaf steps” surface the most relevant passages even if the exact wording differs.

For the generation step we provided two paths. The cloud path uses OpenAI’s GPT‑4o mini model for both text generation and TTS: the script realtime_for_raspberry.py records a user’s voice, transcribes it with gpt‑4o‑transcribe, builds a concise, citation‑aware prompt, sends it to GPT‑4o‑mini for an answer and then reads the result aloud with the built‑in TTS. This approach yields very fluent, natural answers and high‑quality speech, but it requires an API key and internet access. The fully offline path uses a quantised gguf build of an instruction‑tuned model such as Mistral‑7B‑Instruct or Llama‑2‑7B‑Chat, loaded via llama-cpp-python into the Pi’s memory. We crafted a prompt that instructs the model to answer in plain English, limit itself to three or four sentences and include source citations; the notebook generate_hajj_answersـw_llm.ipynb demonstrates this end‑to‑end. Local generation ensures privacy and independence but runs more slowly and produces slightly less polished responses. Overall, the pipeline shows that by combining efficient chunking, a purpose‑built retriever, a lightweight vector store and a quantised local LLM, you can build a smart Islamic RAG system that delivers concise, audible answers on small hardware; and when network access is available, swapping in GPT‑4o via the API yields even faster and more articulate output.

In the interactive voice mode we also integrated a wake‑word detector so that the system only starts recording when you deliberately summon it. Instead of continuously listening or requiring a manual button press, the script monitors the microphone for a short keyword (for example “safa”) using a lightweight wake‑word engine; once the wake word is recognised, it triggers the recording and transcription step. This makes the assistant feel natural—you can be hands‑free and privacy is preserved because no audio is processed until the wake word is spoken.