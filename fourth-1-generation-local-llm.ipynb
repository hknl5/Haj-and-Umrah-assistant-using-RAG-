{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12721500,"sourceType":"datasetVersion","datasetId":8040723}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"e069b078","cell_type":"code","source":"# Install required packages (if not already installed)\n!pip install --quiet chromadb transformers torch llama-cpp-python\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T22:53:02.501888Z","iopub.execute_input":"2025-08-09T22:53:02.502173Z","iopub.status.idle":"2025-08-09T22:56:47.187657Z","shell.execute_reply.started":"2025-08-09T22:53:02.502153Z","shell.execute_reply":"2025-08-09T22:56:47.186833Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.6/19.6 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.1/321.1 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 6.31.1 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 6.31.1 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.31.1 which is incompatible.\ngoogle-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\ndataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.31.1 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"id":"0803595c","cell_type":"code","source":"\nimport os\nos.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n\nimport json\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\nimport chromadb\nfrom llama_cpp import Llama\n\n# Config for embedding model and Chroma\nMODEL_NAME = 'intfloat/e5-base-v2'\nCHROMA_PATH = '/kaggle/input/database-haj/hajj_e5_chroma_backup'\nCOLLECTION_NAME = 'hajj_e5'\nPASSAGE_PREFIX = 'passage: '\nQUERY_PREFIX = 'query: '\nJSON_PATH = '/kaggle/input/database-haj/hajj_chunks_e5.json'\nNPY_PATH = '/kaggle/input/database-haj/hajj_embeddings_e5.npy'\n\n# Path to your quantised LLM file (gguf format)\n#LLM_PATH = '/path/to/mistral-7b-instruct-q4_k_m.gguf'  # TODO: replace with actual path on your Pi\n\n# Maximum tokens for generation and context\nMAX_TOKENS = 256\n\n# Device selection\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T22:56:47.189295Z","iopub.execute_input":"2025-08-09T22:56:47.189560Z","iopub.status.idle":"2025-08-09T22:56:55.247670Z","shell.execute_reply.started":"2025-08-09T22:56:47.189534Z","shell.execute_reply":"2025-08-09T22:56:55.247057Z"}},"outputs":[],"execution_count":2},{"id":"8227b2fc","cell_type":"code","source":"\n# Load E5 model and tokenizer for query encoding\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModel.from_pretrained(MODEL_NAME).to(device)\nmodel.eval()\n\ndef embed_query(text: str):\n    \"\"\"Encode a query string into an embedding vector using E5 and normalise it.\"\"\"\n    input_text = QUERY_PREFIX + text\n    encoded = tokenizer(input_text, return_tensors='pt', truncation=True, max_length=512)\n    encoded = {k: v.to(device) for k, v in encoded.items()}\n    with torch.no_grad():\n        out = model(**encoded)\n        token_embeds = out.last_hidden_state\n        mask = encoded['attention_mask'].unsqueeze(-1)\n        sum_embeds = (token_embeds * mask).sum(dim=1)\n        sum_mask = mask.sum(dim=1)\n        embed = (sum_embeds / sum_mask).squeeze(0).cpu().numpy()\n    norm = np.linalg.norm(embed)\n    if norm > 0:\n        embed = embed / norm\n    return embed\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T23:02:27.196454Z","iopub.execute_input":"2025-08-09T23:02:27.196811Z","iopub.status.idle":"2025-08-09T23:02:27.759670Z","shell.execute_reply.started":"2025-08-09T23:02:27.196787Z","shell.execute_reply":"2025-08-09T23:02:27.758825Z"}},"outputs":[],"execution_count":5},{"id":"480376f5","cell_type":"code","source":"\n# Copy the folder into a writable location (if it came from a read-only dataset)\nimport shutil\nshutil.copytree('/kaggle/input/database-haj/hajj_e5_chroma_backup', '/kaggle/working/hajj_e5_chroma')\n\n# Then point Chroma at the copy\nCHROMA_PATH = '/kaggle/working/hajj_e5_chroma'\nclient = chromadb.PersistentClient(path=CHROMA_PATH)\ncollection = client.get_or_create_collection(name='hajj_e5', metadata={'hnsw:space': 'cosine'})\n\n# Helper search function with lexical re-ranking as fallback\ndef search(query_str: str, top_k: int = 10, re_rank: bool = True):\n    query_embed = embed_query(query_str)\n    result = collection.query(query_embeddings=[query_embed.tolist()], n_results=top_k)\n    ids = result['ids'][0]\n    dists = result['distances'][0]\n    docs = result['documents'][0]\n    metas = result['metadatas'][0]\n    hits = []\n    for id_, dist, doc, meta in zip(ids, dists, docs, metas):\n        hits.append({'id': id_, 'distance': float(dist), 'text': doc, 'metadata': meta})\n    if re_rank:\n        query_tokens = set(query_str.lower().split())\n        for h in hits:\n            text_tokens = set(h['text'].lower().split())\n            h['lexical_score'] = len(query_tokens & text_tokens)\n        hits.sort(key=lambda x: x['lexical_score'], reverse=True)\n    return hits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T23:03:30.978421Z","iopub.execute_input":"2025-08-09T23:03:30.978847Z","iopub.status.idle":"2025-08-09T23:03:31.501827Z","shell.execute_reply.started":"2025-08-09T23:03:30.978817Z","shell.execute_reply":"2025-08-09T23:03:31.501224Z"}},"outputs":[],"execution_count":7},{"id":"da13e72c","cell_type":"code","source":"\ndef build_prompt(question: str, sources: list):\n    \"\"\"Construct a prompt for the LLM using the question and retrieved sources.\"\"\"\n    prompt_lines = []\n    prompt_lines.append(\"You are an assistant answering questions about Hajj and Umrah.\")\n    prompt_lines.append(\"Answer concisely in plain English so that the response can be read aloud.\")\n    prompt_lines.append(\"Keep the answer to no more than 3–4 sentences.\")\n    prompt_lines.append(f\"Question: {question}\")\n    prompt_lines.append(\"Sources:\")\n    for i, src in enumerate(sources, 1):\n        text = src['text'].replace(\"\", \" \").strip()\n        if len(text) > 300:\n            text = text[:297] + '...'\n        prompt_lines.append(f\"[{i}] {text}\")\n    prompt_lines.append(\"Answer:\")\n    return \"\".join(prompt_lines)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T23:19:33.588754Z","iopub.execute_input":"2025-08-09T23:19:33.589361Z","iopub.status.idle":"2025-08-09T23:19:33.594705Z","shell.execute_reply.started":"2025-08-09T23:19:33.589335Z","shell.execute_reply":"2025-08-09T23:19:33.593824Z"}},"outputs":[],"execution_count":27},{"id":"f00b3a5b-0454-4adf-aad1-357fd6cd5ae7","cell_type":"code","source":"!pip -q install llama-cpp-python huggingface_hub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T23:11:34.216439Z","iopub.execute_input":"2025-08-09T23:11:34.216723Z","iopub.status.idle":"2025-08-09T23:11:37.726854Z","shell.execute_reply.started":"2025-08-09T23:11:34.216706Z","shell.execute_reply":"2025-08-09T23:11:37.725839Z"}},"outputs":[],"execution_count":16},{"id":"84e31a6f-7bcf-4f47-a096-da2a794a3060","cell_type":"code","source":"from huggingface_hub import hf_hub_download, list_repo_files\n\nrepo = \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n\nfiles = [f for f in list_repo_files(repo) if f.lower().endswith(\".gguf\")]\ncandidates = [f for f in files if \"q4_k_m\" in f.lower()]\n\nif not candidates:\n    raise RuntimeError(\"No Q4_K_M .gguf found in the repo. Available: \" + \", \".join(files))\n\nfilename = candidates[0]  \nprint(\"Selected file:\", filename)\n\nLLM_PATH = hf_hub_download(repo_id=repo, filename=filename)\nprint(\"Downloaded to:\", LLM_PATH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T23:15:11.026956Z","iopub.execute_input":"2025-08-09T23:15:11.027521Z","iopub.status.idle":"2025-08-09T23:15:11.114715Z","shell.execute_reply.started":"2025-08-09T23:15:11.027494Z","shell.execute_reply":"2025-08-09T23:15:11.114054Z"}},"outputs":[{"name":"stdout","text":"Selected file: mistral-7b-instruct-v0.2.Q4_K_M.gguf\nDownloaded to: /root/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.2-GGUF/snapshots/3a6fbf4a41a1d52e415a4958cde6856d34b2db93/mistral-7b-instruct-v0.2.Q4_K_M.gguf\n","output_type":"stream"}],"execution_count":23},{"id":"ee6daa51","cell_type":"code","source":"\n# Load quantised local model using llama-cpp-python\n# Ensure the .gguf model file exists at LLM_PATH on your Raspberry Pi\nllm = Llama(model_path=LLM_PATH, n_ctx=2048)\n\ndef generate_answer(question: str, top_k: int = 5):\n    \"\"\"Retrieve sources and generate an answer using the local LLM.\"\"\"\n    hits = search(question, top_k=top_k, re_rank=True)\n    prompt = build_prompt(question, hits)\n    result = llm(prompt, max_tokens=MAX_TOKENS, temperature=0.2, top_p=0.95, stop=[\"Sources:\", \"Question:\"])\n    answer = result['choices'][0]['text'].strip()\n    return answer, hits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T23:15:20.171624Z","iopub.execute_input":"2025-08-09T23:15:20.172338Z","iopub.status.idle":"2025-08-09T23:15:25.748490Z","shell.execute_reply.started":"2025-08-09T23:15:20.172312Z","shell.execute_reply":"2025-08-09T23:15:25.747747Z"}},"outputs":[{"name":"stderr","text":"llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.2-GGUF/snapshots/3a6fbf4a41a1d52e415a4958cde6856d34b2db93/mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\nllama_model_loader: - kv   2:                       llama.context_length u32              = 32768\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  11:                          general.file_type u32              = 15\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\nllama_model_loader: - kv  23:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type q4_K:  193 tensors\nllama_model_loader: - type q6_K:   33 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 4.07 GiB (4.83 BPW) \ninit_tokenizer: initializing tokenizer for type 1\nload: control token:      2 '</s>' is not marked as EOG\nload: control token:      1 '<s>' is not marked as EOG\nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: printing all EOG tokens:\nload:   - 2 ('</s>')\nload: special tokens cache size = 3\nload: token to piece cache size = 0.1637 MB\nprint_info: arch             = llama\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 32768\nprint_info: n_embd           = 4096\nprint_info: n_layer          = 32\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: is_swa_any       = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 4\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-05\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 14336\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 0\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 32768\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 7B\nprint_info: model params     = 7.24 B\nprint_info: general.name     = mistralai_mistral-7b-instruct-v0.2\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 32000\nprint_info: n_merges         = 0\nprint_info: BOS token        = 1 '<s>'\nprint_info: EOS token        = 2 '</s>'\nprint_info: UNK token        = 0 '<unk>'\nprint_info: PAD token        = 0 '<unk>'\nprint_info: LF token         = 13 '<0x0A>'\nprint_info: EOG token        = 2 '</s>'\nprint_info: max token length = 48\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: layer   0 assigned to device CPU, is_swa = 0\nload_tensors: layer   1 assigned to device CPU, is_swa = 0\nload_tensors: layer   2 assigned to device CPU, is_swa = 0\nload_tensors: layer   3 assigned to device CPU, is_swa = 0\nload_tensors: layer   4 assigned to device CPU, is_swa = 0\nload_tensors: layer   5 assigned to device CPU, is_swa = 0\nload_tensors: layer   6 assigned to device CPU, is_swa = 0\nload_tensors: layer   7 assigned to device CPU, is_swa = 0\nload_tensors: layer   8 assigned to device CPU, is_swa = 0\nload_tensors: layer   9 assigned to device CPU, is_swa = 0\nload_tensors: layer  10 assigned to device CPU, is_swa = 0\nload_tensors: layer  11 assigned to device CPU, is_swa = 0\nload_tensors: layer  12 assigned to device CPU, is_swa = 0\nload_tensors: layer  13 assigned to device CPU, is_swa = 0\nload_tensors: layer  14 assigned to device CPU, is_swa = 0\nload_tensors: layer  15 assigned to device CPU, is_swa = 0\nload_tensors: layer  16 assigned to device CPU, is_swa = 0\nload_tensors: layer  17 assigned to device CPU, is_swa = 0\nload_tensors: layer  18 assigned to device CPU, is_swa = 0\nload_tensors: layer  19 assigned to device CPU, is_swa = 0\nload_tensors: layer  20 assigned to device CPU, is_swa = 0\nload_tensors: layer  21 assigned to device CPU, is_swa = 0\nload_tensors: layer  22 assigned to device CPU, is_swa = 0\nload_tensors: layer  23 assigned to device CPU, is_swa = 0\nload_tensors: layer  24 assigned to device CPU, is_swa = 0\nload_tensors: layer  25 assigned to device CPU, is_swa = 0\nload_tensors: layer  26 assigned to device CPU, is_swa = 0\nload_tensors: layer  27 assigned to device CPU, is_swa = 0\nload_tensors: layer  28 assigned to device CPU, is_swa = 0\nload_tensors: layer  29 assigned to device CPU, is_swa = 0\nload_tensors: layer  30 assigned to device CPU, is_swa = 0\nload_tensors: layer  31 assigned to device CPU, is_swa = 0\nload_tensors: layer  32 assigned to device CPU, is_swa = 0\nload_tensors: tensor 'token_embd.weight' (q4_K) (and 98 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\nload_tensors:   CPU_REPACK model buffer size =  3204.00 MiB\nload_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\nrepack: repack tensor blk.0.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.0.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.0.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.0.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.1.attn_q.weight with q4_K_8x8\n.repack: repack tensor blk.1.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.1.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.1.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.2.attn_q.weight with q4_K_8x8\n.repack: repack tensor blk.2.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.2.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.2.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.3.attn_q.weight with q4_K_8x8\n.repack: repack tensor blk.3.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.3.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.3.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.4.attn_q.weight with q4_K_8x8\n.repack: repack tensor blk.4.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.4.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.4.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.4.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.4.ffn_down.weight with q4_K_8x8\n.repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.5.attn_q.weight with q4_K_8x8\n.repack: repack tensor blk.5.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.5.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.5.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.5.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.5.ffn_down.weight with q4_K_8x8\nrepack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.6.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.6.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n.repack: repack tensor blk.6.ffn_gate.weight with q4_K_8x8\nrepack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.7.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.7.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.7.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n.repack: repack tensor blk.7.ffn_gate.weight with q4_K_8x8\nrepack: repack tensor blk.7.ffn_down.weight with q4_K_8x8\n.repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.8.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.8.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.8.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.8.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.8.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.8.ffn_down.weight with q4_K_8x8\n.repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.9.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.9.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.9.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.9.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.10.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.10.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.10.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.10.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.10.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.10.ffn_down.weight with q4_K_8x8\n.repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.11.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.11.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.11.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.11.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.11.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.11.ffn_down.weight with q4_K_8x8\n.repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.12.attn_q.weight with q4_K_8x8\n.repack: repack tensor blk.12.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.12.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.12.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.13.attn_q.weight with q4_K_8x8\n.repack: repack tensor blk.13.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.13.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.13.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.13.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.13.ffn_down.weight with q4_K_8x8\nrepack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.14.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.14.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.14.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n.repack: repack tensor blk.14.ffn_gate.weight with q4_K_8x8\nrepack: repack tensor blk.14.ffn_down.weight with q4_K_8x8\n.repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.15.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.15.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.15.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.15.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.16.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.16.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.16.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n.repack: repack tensor blk.16.ffn_gate.weight with q4_K_8x8\nrepack: repack tensor blk.16.ffn_down.weight with q4_K_8x8\n.repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.17.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.17.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.17.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.17.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.17.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.17.ffn_down.weight with q4_K_8x8\n.repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.18.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.18.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.18.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.18.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.19.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.19.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.19.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.19.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.19.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.19.ffn_down.weight with q4_K_8x8\n.repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.20.attn_q.weight with q4_K_8x8\n.repack: repack tensor blk.20.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.20.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.20.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.20.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.20.ffn_down.weight with q4_K_8x8\nrepack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.21.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.21.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n.repack: repack tensor blk.21.ffn_gate.weight with q4_K_8x8\nrepack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.22.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.22.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.22.attn_v.weight with q4_K_8x8\n.repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.22.ffn_gate.weight with q4_K_8x8\nrepack: repack tensor blk.22.ffn_down.weight with q4_K_8x8\n.repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.23.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.23.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.23.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n.repack: repack tensor blk.23.ffn_gate.weight with q4_K_8x8\nrepack: repack tensor blk.23.ffn_down.weight with q4_K_8x8\n.repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.24.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.24.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.24.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.24.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.25.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.25.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.25.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.25.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.25.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.25.ffn_down.weight with q4_K_8x8\n.repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.26.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.26.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.26.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.26.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.26.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.26.ffn_down.weight with q4_K_8x8\n.repack: repack tensor blk.26.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.27.attn_q.weight with q4_K_8x8\n.repack: repack tensor blk.27.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.27.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.27.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.27.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.28.attn_q.weight with q4_K_8x8\n.repack: repack tensor blk.28.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.28.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.28.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.28.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.29.attn_q.weight with q4_K_8x8\n.repack: repack tensor blk.29.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.29.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.29.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.29.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.30.attn_q.weight with q4_K_8x8\n.repack: repack tensor blk.30.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.30.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.30.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.30.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.31.attn_q.weight with q4_K_8x8\n.repack: repack tensor blk.31.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.31.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.31.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.31.ffn_up.weight with q4_K_8x8\n...................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 2048\nllama_context: n_ctx_per_seq = 2048\nllama_context: n_batch       = 512\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 0\nllama_context: kv_unified    = false\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\nset_abort_callback: call\nllama_context:        CPU  output buffer size =     0.12 MiB\ncreate_memory: n_ctx = 2048 (padded)\nllama_kv_cache_unified: layer   0: dev = CPU\nllama_kv_cache_unified: layer   1: dev = CPU\nllama_kv_cache_unified: layer   2: dev = CPU\nllama_kv_cache_unified: layer   3: dev = CPU\nllama_kv_cache_unified: layer   4: dev = CPU\nllama_kv_cache_unified: layer   5: dev = CPU\nllama_kv_cache_unified: layer   6: dev = CPU\nllama_kv_cache_unified: layer   7: dev = CPU\nllama_kv_cache_unified: layer   8: dev = CPU\nllama_kv_cache_unified: layer   9: dev = CPU\nllama_kv_cache_unified: layer  10: dev = CPU\nllama_kv_cache_unified: layer  11: dev = CPU\nllama_kv_cache_unified: layer  12: dev = CPU\nllama_kv_cache_unified: layer  13: dev = CPU\nllama_kv_cache_unified: layer  14: dev = CPU\nllama_kv_cache_unified: layer  15: dev = CPU\nllama_kv_cache_unified: layer  16: dev = CPU\nllama_kv_cache_unified: layer  17: dev = CPU\nllama_kv_cache_unified: layer  18: dev = CPU\nllama_kv_cache_unified: layer  19: dev = CPU\nllama_kv_cache_unified: layer  20: dev = CPU\nllama_kv_cache_unified: layer  21: dev = CPU\nllama_kv_cache_unified: layer  22: dev = CPU\nllama_kv_cache_unified: layer  23: dev = CPU\nllama_kv_cache_unified: layer  24: dev = CPU\nllama_kv_cache_unified: layer  25: dev = CPU\nllama_kv_cache_unified: layer  26: dev = CPU\nllama_kv_cache_unified: layer  27: dev = CPU\nllama_kv_cache_unified: layer  28: dev = CPU\nllama_kv_cache_unified: layer  29: dev = CPU\nllama_kv_cache_unified: layer  30: dev = CPU\nllama_kv_cache_unified: layer  31: dev = CPU\nllama_kv_cache_unified:        CPU KV buffer size =   256.00 MiB\nllama_kv_cache_unified: size =  256.00 MiB (  2048 cells,  32 layers,  1/1 seqs), K (f16):  128.00 MiB, V (f16):  128.00 MiB\nllama_context: enumerating backends\nllama_context: backend_ptrs.size() = 1\nllama_context: max_nodes = 2328\nllama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\ngraph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\ngraph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\ngraph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\nllama_context:        CPU compute buffer size =   168.01 MiB\nllama_context: graph nodes  = 1126\nllama_context: graph splits = 1\nCPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \nModel metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\nAvailable chat formats from metadata: chat_template.default\nGuessed chat format: mistral-instruct\n","output_type":"stream"}],"execution_count":24},{"id":"2b3d9e29","cell_type":"code","source":"\n# Example question\nquestion = \"What are the steps of Umrah?\"\nanswer, sources = generate_answer(question, top_k=5)\nprint(\"Answer:\", answer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T23:20:03.900539Z","iopub.execute_input":"2025-08-09T23:20:03.901088Z","iopub.status.idle":"2025-08-09T23:21:49.362658Z","shell.execute_reply.started":"2025-08-09T23:20:03.901058Z","shell.execute_reply":"2025-08-09T23:21:49.361829Z"}},"outputs":[{"name":"stderr","text":"Llama.generate: 45 prefix-match hit, remaining 781 prompt tokens to eval\nllama_perf_context_print:        load time =   86818.14 ms\nllama_perf_context_print: prompt eval time =   79613.16 ms /   781 tokens (  101.94 ms per token,     9.81 tokens per second)\nllama_perf_context_print:        eval time =   25778.43 ms /    76 runs   (  339.19 ms per token,     2.95 tokens per second)\nllama_perf_context_print:       total time =  105436.06 ms /   857 tokens\nllama_perf_context_print:    graphs reused =         72\n","output_type":"stream"},{"name":"stdout","text":"Answer: The steps of Umrah include wearing Ihram at the Miqat, entering the holy area of Masjid Al Haram, performing Tawaf around the Kaaba seven times, and performing Sa'ee between Safa and Marwa seven times. Additionally, there are official requirements such as identity verification and permit issuance to perform Umrah at specific times.\n","output_type":"stream"}],"execution_count":28},{"id":"78fc7a19-0803-4fde-9fec-918344eb2575","cell_type":"code","source":"print(\"Sources used:\")\n\nfor i, src in enumerate(sources, 1):\n    print(f\"[{i}] {src['text'][:150].replace('','')}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T23:21:49.370796Z","iopub.execute_input":"2025-08-09T23:21:49.371293Z","iopub.status.idle":"2025-08-09T23:21:49.386861Z","shell.execute_reply.started":"2025-08-09T23:21:49.371266Z","shell.execute_reply":"2025-08-09T23:21:49.386084Z"}},"outputs":[{"name":"stdout","text":"Sources used:\n[1] ##jj and umrah consecutively ; for they remove poverty and sin as the bellows removes impurity from iron. \" - repeating umrah expiates the sins commit\n[2] umrah, the pilgrim turns toward the house of allah with his heart, tongue, limbs, and seeks his mercy and pleasure. umrah is an act of worshipping all\n[3] for umrah in the hajj - months ( shawwal, dhul - qi'dah and dhul - hijjah ) - when he reaches makkah, he performs tawaf and sa'i for his umrah, shaves\n[4] official for exact umrah timing - know your residence location and save the address - remember your bus stop and meeting point - check gate panels for\n[5] 2. identity verification 3. permit issued according to available date must perform umrah at specified times shown in permit. choosing less crowded tim\n","output_type":"stream"}],"execution_count":30},{"id":"0abe9f98-0619-4c96-95c5-48ba2a283f5d","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}